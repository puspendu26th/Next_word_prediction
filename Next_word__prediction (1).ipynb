{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcpbuoMzgH2N"
      },
      "source": [
        "**Next_Word_Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbaehJyOgSYH"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K4qJtUWdn81o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b84f670-f47e-4876-bc3a-44df05e1acd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python_version>\"3.7\"\n",
            "  Downloading python_version-0.0.2-py2.py3-none-any.whl (3.4 kB)\n",
            "Building wheels for collected packages: tensorflow-gpu\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tensorflow-gpu (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for tensorflow-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for tensorflow-gpu\n",
            "Failed to build tensorflow-gpu\n",
            "Installing collected packages: python_version, tensorflow-gpu\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for tensorflow-gpu\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Running setup.py install for tensorflow-gpu ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m╰─>\u001b[0m tensorflow-gpu\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "pLNyhHOYiy7q",
        "outputId": "ae8a95dc-3474-4106-9724-83154b093f97"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b6ec0f57-ecce-4a56-b891-9ea8269a6c0a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b6ec0f57-ecce-4a56-b891-9ea8269a6c0a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving text_dataset.txt to text_dataset.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4fHACrljbWs"
      },
      "source": [
        "# Load and PreProcess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "x9_zhm3hji8M",
        "outputId": "644df3e6-dc12-4160-b815-45f5fe4796a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of A Room With A View, by E. M. Forster This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "file=open(\"text_dataset.txt\",\"r\",encoding=\"utf8\")\n",
        "#store file in list\n",
        "line=[]\n",
        "for i in file:\n",
        "  line.append(i)\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in line:\n",
        "  data = ' '. join(line)\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  \n",
        "#remove unnecessary spaces \n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5VthGGCu2LN",
        "outputId": "d8c5ad4d-11be-4954-d956-cd327be0d644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "387734"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX1d1sZNvMkd"
      },
      "source": [
        "# Apply tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPSu_sObvN_P",
        "outputId": "e0a293d8-26b1-481e-c1c7-62c1f0862f12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 114, 105, 573, 4, 5, 115, 19, 5, 195, 46, 310, 1223, 1839, 33]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(token, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = token.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGypV_0PvfUk",
        "outputId": "4ebd33fa-2c9a-4106-bcdd-3bcd421f1bf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70167"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(sequence_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWealIW0w9Gw",
        "outputId": "e0bbfd8c-b216-4f0d-c7cc-20e3858fa15b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7866"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "vocab_size = len(token.word_index) + 1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSCRpx620JqD",
        "outputId": "235beec6-6e36-487b-94ce-a37f89a4dc24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  70164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,  114,  105,  573],\n",
              "       [ 114,  105,  573,    4],\n",
              "       [ 105,  573,    4,    5],\n",
              "       [ 573,    4,    5,  115],\n",
              "       [   4,    5,  115,   19],\n",
              "       [   5,  115,   19,    5],\n",
              "       [ 115,   19,    5,  195],\n",
              "       [  19,    5,  195,   46],\n",
              "       [   5,  195,   46,  310],\n",
              "       [ 195,   46,  310, 1223],\n",
              "       [  46,  310, 1223, 1839],\n",
              "       [ 310, 1223, 1839,   33],\n",
              "       [1223, 1839,   33,  573],\n",
              "       [1839,   33,  573,   17],\n",
              "       [  33,  573,   17,   20]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gqox9G-Y0oXP"
      },
      "outputs": [],
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    x.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "    \n",
        "X = np.array(x)\n",
        "y = np.array(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKM43k_h1PIa",
        "outputId": "0caade29-12ce-4f33-9cdf-2def6d5c6d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [array([  1, 114, 105]), array([114, 105, 573]), array([105, 573,   4]), array([573,   4,   5]), array([  4,   5, 115]), array([  5, 115,  19]), array([115,  19,   5]), array([ 19,   5, 195]), array([  5, 195,  46]), array([195,  46, 310])]\n",
            "Response:  [ 573    4    5  115   19    5  195   46  310 1223]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data: \", x[:10])\n",
        "print(\"Response: \", y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8zzNPz-2TjJ",
        "outputId": "b048170c-73e2-47e1-e2fa-5e044f1eca60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eei4BWw3sfD"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xPcs3FCE3w8r"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SandCfxP4HJK",
        "outputId": "c4970112-2431-46c5-8aa6-bc0865aebb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             78660     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7866)              7873866   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,001,526\n",
            "Trainable params: 21,001,526\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwWxuPuH5pU_"
      },
      "source": [
        "# Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGUPtXuP5ofK",
        "outputId": "d820e81f-c443-4cb8-955f-9a17b0f06bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 6.7470 - accuracy: 0.0471\n",
            "Epoch 1: loss improved from inf to 6.74704, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 40s 29ms/step - loss: 6.7470 - accuracy: 0.0471\n",
            "Epoch 2/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 6.2108 - accuracy: 0.0718\n",
            "Epoch 2: loss improved from 6.74704 to 6.21081, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 6.2108 - accuracy: 0.0718\n",
            "Epoch 3/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 5.8002 - accuracy: 0.0955\n",
            "Epoch 3: loss improved from 6.21081 to 5.80033, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 5.8003 - accuracy: 0.0955\n",
            "Epoch 4/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 5.4985 - accuracy: 0.1096\n",
            "Epoch 4: loss improved from 5.80033 to 5.49856, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 5.4986 - accuracy: 0.1096\n",
            "Epoch 5/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 5.2685 - accuracy: 0.1199\n",
            "Epoch 5: loss improved from 5.49856 to 5.26849, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 5.2685 - accuracy: 0.1199\n",
            "Epoch 6/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 5.0606 - accuracy: 0.1290\n",
            "Epoch 6: loss improved from 5.26849 to 5.06063, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 5.0606 - accuracy: 0.1290\n",
            "Epoch 7/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 4.8462 - accuracy: 0.1395\n",
            "Epoch 7: loss improved from 5.06063 to 4.84616, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 4.8462 - accuracy: 0.1395\n",
            "Epoch 8/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 4.6201 - accuracy: 0.1511\n",
            "Epoch 8: loss improved from 4.84616 to 4.61994, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 4.6199 - accuracy: 0.1511\n",
            "Epoch 9/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 4.3734 - accuracy: 0.1620\n",
            "Epoch 9: loss improved from 4.61994 to 4.37339, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 4.3734 - accuracy: 0.1620\n",
            "Epoch 10/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 4.1108 - accuracy: 0.1804\n",
            "Epoch 10: loss improved from 4.37339 to 4.11076, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 4.1108 - accuracy: 0.1804\n",
            "Epoch 11/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 3.8410 - accuracy: 0.2064\n",
            "Epoch 11: loss improved from 4.11076 to 3.84117, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 3.8412 - accuracy: 0.2064\n",
            "Epoch 12/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 3.5723 - accuracy: 0.2378\n",
            "Epoch 12: loss improved from 3.84117 to 3.57229, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 3.5723 - accuracy: 0.2378\n",
            "Epoch 13/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 3.3203 - accuracy: 0.2731\n",
            "Epoch 13: loss improved from 3.57229 to 3.32066, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 3.3207 - accuracy: 0.2731\n",
            "Epoch 14/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 3.0811 - accuracy: 0.3082\n",
            "Epoch 14: loss improved from 3.32066 to 3.08113, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 3.0811 - accuracy: 0.3082\n",
            "Epoch 15/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 2.8586 - accuracy: 0.3450\n",
            "Epoch 15: loss improved from 3.08113 to 2.85876, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 2.8588 - accuracy: 0.3450\n",
            "Epoch 16/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 2.6490 - accuracy: 0.3815\n",
            "Epoch 16: loss improved from 2.85876 to 2.64945, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 2.6494 - accuracy: 0.3815\n",
            "Epoch 17/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 2.4427 - accuracy: 0.4163\n",
            "Epoch 17: loss improved from 2.64945 to 2.44266, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 2.4427 - accuracy: 0.4163\n",
            "Epoch 18/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 2.2480 - accuracy: 0.4532\n",
            "Epoch 18: loss improved from 2.44266 to 2.24798, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 2.2480 - accuracy: 0.4532\n",
            "Epoch 19/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 2.0601 - accuracy: 0.4900\n",
            "Epoch 19: loss improved from 2.24798 to 2.06009, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 2.0601 - accuracy: 0.4900\n",
            "Epoch 20/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 1.8668 - accuracy: 0.5278\n",
            "Epoch 20: loss improved from 2.06009 to 1.86676, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 1.8668 - accuracy: 0.5278\n",
            "Epoch 21/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 1.6920 - accuracy: 0.5668\n",
            "Epoch 21: loss improved from 1.86676 to 1.69182, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 1.6918 - accuracy: 0.5668\n",
            "Epoch 22/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.6051\n",
            "Epoch 22: loss improved from 1.69182 to 1.52079, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 1.5208 - accuracy: 0.6052\n",
            "Epoch 23/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 1.3567 - accuracy: 0.6426\n",
            "Epoch 23: loss improved from 1.52079 to 1.35684, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 1.3568 - accuracy: 0.6425\n",
            "Epoch 24/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 1.2003 - accuracy: 0.6822\n",
            "Epoch 24: loss improved from 1.35684 to 1.20066, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 1.2007 - accuracy: 0.6821\n",
            "Epoch 25/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 1.0675 - accuracy: 0.7161\n",
            "Epoch 25: loss improved from 1.20066 to 1.06754, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 1.0675 - accuracy: 0.7161\n",
            "Epoch 26/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.9519 - accuracy: 0.7452\n",
            "Epoch 26: loss improved from 1.06754 to 0.95214, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.9521 - accuracy: 0.7451\n",
            "Epoch 27/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.8464 - accuracy: 0.7733\n",
            "Epoch 27: loss improved from 0.95214 to 0.84638, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.8464 - accuracy: 0.7733\n",
            "Epoch 28/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.7635 - accuracy: 0.7957\n",
            "Epoch 28: loss improved from 0.84638 to 0.76347, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.7635 - accuracy: 0.7957\n",
            "Epoch 29/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.8155\n",
            "Epoch 29: loss improved from 0.76347 to 0.69267, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.6927 - accuracy: 0.8154\n",
            "Epoch 30/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.8296\n",
            "Epoch 30: loss improved from 0.69267 to 0.63904, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.6390 - accuracy: 0.8296\n",
            "Epoch 31/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.5976 - accuracy: 0.8402\n",
            "Epoch 31: loss improved from 0.63904 to 0.59764, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.5976 - accuracy: 0.8402\n",
            "Epoch 32/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.5588 - accuracy: 0.8499\n",
            "Epoch 32: loss improved from 0.59764 to 0.55886, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.5589 - accuracy: 0.8499\n",
            "Epoch 33/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.5268 - accuracy: 0.8580\n",
            "Epoch 33: loss improved from 0.55886 to 0.52676, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.5268 - accuracy: 0.8580\n",
            "Epoch 34/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.4992 - accuracy: 0.8652\n",
            "Epoch 34: loss improved from 0.52676 to 0.49927, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.4993 - accuracy: 0.8651\n",
            "Epoch 35/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.8686\n",
            "Epoch 35: loss improved from 0.49927 to 0.47817, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.4782 - accuracy: 0.8686\n",
            "Epoch 36/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.8693\n",
            "Epoch 36: loss improved from 0.47817 to 0.46584, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.4658 - accuracy: 0.8693\n",
            "Epoch 37/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8738\n",
            "Epoch 37: loss improved from 0.46584 to 0.44795, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.4479 - accuracy: 0.8738\n",
            "Epoch 38/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8785\n",
            "Epoch 38: loss improved from 0.44795 to 0.42640, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.4264 - accuracy: 0.8785\n",
            "Epoch 39/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8789\n",
            "Epoch 39: loss improved from 0.42640 to 0.42155, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.4215 - accuracy: 0.8789\n",
            "Epoch 40/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.8809\n",
            "Epoch 40: loss improved from 0.42155 to 0.40667, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.4067 - accuracy: 0.8808\n",
            "Epoch 41/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3982 - accuracy: 0.8824\n",
            "Epoch 41: loss improved from 0.40667 to 0.39816, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3982 - accuracy: 0.8824\n",
            "Epoch 42/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8807\n",
            "Epoch 42: loss improved from 0.39816 to 0.39561, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3956 - accuracy: 0.8807\n",
            "Epoch 43/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8837\n",
            "Epoch 43: loss improved from 0.39561 to 0.38072, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3807 - accuracy: 0.8837\n",
            "Epoch 44/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.8852\n",
            "Epoch 44: loss improved from 0.38072 to 0.37329, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3733 - accuracy: 0.8852\n",
            "Epoch 45/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.3617 - accuracy: 0.8866\n",
            "Epoch 45: loss improved from 0.37329 to 0.36179, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3618 - accuracy: 0.8866\n",
            "Epoch 46/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8849\n",
            "Epoch 46: loss did not improve from 0.36179\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3631 - accuracy: 0.8849\n",
            "Epoch 47/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8855\n",
            "Epoch 47: loss improved from 0.36179 to 0.35667, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.3567 - accuracy: 0.8855\n",
            "Epoch 48/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.8864\n",
            "Epoch 48: loss improved from 0.35667 to 0.34738, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3474 - accuracy: 0.8864\n",
            "Epoch 49/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8880\n",
            "Epoch 49: loss improved from 0.34738 to 0.33979, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3398 - accuracy: 0.8880\n",
            "Epoch 50/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8893\n",
            "Epoch 50: loss improved from 0.33979 to 0.33524, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3352 - accuracy: 0.8894\n",
            "Epoch 51/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.8893\n",
            "Epoch 51: loss improved from 0.33524 to 0.33095, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.3310 - accuracy: 0.8893\n",
            "Epoch 52/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8889\n",
            "Epoch 52: loss improved from 0.33095 to 0.32810, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3281 - accuracy: 0.8889\n",
            "Epoch 53/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.8892\n",
            "Epoch 53: loss improved from 0.32810 to 0.32313, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3231 - accuracy: 0.8892\n",
            "Epoch 54/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.8879\n",
            "Epoch 54: loss did not improve from 0.32313\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3245 - accuracy: 0.8879\n",
            "Epoch 55/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8902\n",
            "Epoch 55: loss improved from 0.32313 to 0.31036, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.3104 - accuracy: 0.8902\n",
            "Epoch 56/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.8895\n",
            "Epoch 56: loss did not improve from 0.31036\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3131 - accuracy: 0.8895\n",
            "Epoch 57/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.3044 - accuracy: 0.8916\n",
            "Epoch 57: loss improved from 0.31036 to 0.30441, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3044 - accuracy: 0.8916\n",
            "Epoch 58/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.8894\n",
            "Epoch 58: loss did not improve from 0.30441\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.3049 - accuracy: 0.8894\n",
            "Epoch 59/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.8902\n",
            "Epoch 59: loss improved from 0.30441 to 0.29814, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2981 - accuracy: 0.8903\n",
            "Epoch 60/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2961 - accuracy: 0.8913\n",
            "Epoch 60: loss improved from 0.29814 to 0.29610, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2961 - accuracy: 0.8913\n",
            "Epoch 61/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8896\n",
            "Epoch 61: loss did not improve from 0.29610\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2969 - accuracy: 0.8895\n",
            "Epoch 62/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.8908\n",
            "Epoch 62: loss improved from 0.29610 to 0.29158, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2916 - accuracy: 0.8908\n",
            "Epoch 63/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2927 - accuracy: 0.8903\n",
            "Epoch 63: loss did not improve from 0.29158\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2927 - accuracy: 0.8903\n",
            "Epoch 64/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8909\n",
            "Epoch 64: loss improved from 0.29158 to 0.28827, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2883 - accuracy: 0.8909\n",
            "Epoch 65/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2817 - accuracy: 0.8925\n",
            "Epoch 65: loss improved from 0.28827 to 0.28175, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2817 - accuracy: 0.8925\n",
            "Epoch 66/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.8917\n",
            "Epoch 66: loss improved from 0.28175 to 0.28105, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2810 - accuracy: 0.8917\n",
            "Epoch 67/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8901\n",
            "Epoch 67: loss did not improve from 0.28105\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2849 - accuracy: 0.8901\n",
            "Epoch 68/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2746 - accuracy: 0.8914\n",
            "Epoch 68: loss improved from 0.28105 to 0.27457, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2746 - accuracy: 0.8914\n",
            "Epoch 69/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2740 - accuracy: 0.8911\n",
            "Epoch 69: loss improved from 0.27457 to 0.27405, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2741 - accuracy: 0.8911\n",
            "Epoch 70/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2722 - accuracy: 0.8909\n",
            "Epoch 70: loss improved from 0.27405 to 0.27221, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2722 - accuracy: 0.8909\n",
            "Epoch 71/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8927\n",
            "Epoch 71: loss improved from 0.27221 to 0.26708, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2671 - accuracy: 0.8927\n",
            "Epoch 72/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8923\n",
            "Epoch 72: loss did not improve from 0.26708\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2684 - accuracy: 0.8923\n",
            "Epoch 73/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.8925\n",
            "Epoch 73: loss improved from 0.26708 to 0.26561, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2656 - accuracy: 0.8925\n",
            "Epoch 74/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.8922\n",
            "Epoch 74: loss did not improve from 0.26561\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2658 - accuracy: 0.8922\n",
            "Epoch 75/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.8920\n",
            "Epoch 75: loss improved from 0.26561 to 0.26511, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2651 - accuracy: 0.8920\n",
            "Epoch 76/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2645 - accuracy: 0.8916\n",
            "Epoch 76: loss improved from 0.26511 to 0.26454, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2645 - accuracy: 0.8916\n",
            "Epoch 77/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.8925\n",
            "Epoch 77: loss improved from 0.26454 to 0.25926, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2593 - accuracy: 0.8925\n",
            "Epoch 78/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2570 - accuracy: 0.8920\n",
            "Epoch 78: loss improved from 0.25926 to 0.25717, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2572 - accuracy: 0.8920\n",
            "Epoch 79/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.8923\n",
            "Epoch 79: loss did not improve from 0.25717\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2605 - accuracy: 0.8922\n",
            "Epoch 80/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.8921\n",
            "Epoch 80: loss did not improve from 0.25717\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2590 - accuracy: 0.8921\n",
            "Epoch 81/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.8930\n",
            "Epoch 81: loss improved from 0.25717 to 0.25259, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2526 - accuracy: 0.8929\n",
            "Epoch 82/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.8921\n",
            "Epoch 82: loss did not improve from 0.25259\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2547 - accuracy: 0.8921\n",
            "Epoch 83/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.8935\n",
            "Epoch 83: loss improved from 0.25259 to 0.25065, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2506 - accuracy: 0.8935\n",
            "Epoch 84/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.8943\n",
            "Epoch 84: loss improved from 0.25065 to 0.24530, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2453 - accuracy: 0.8943\n",
            "Epoch 85/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8937\n",
            "Epoch 85: loss improved from 0.24530 to 0.24486, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2449 - accuracy: 0.8937\n",
            "Epoch 86/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.8908\n",
            "Epoch 86: loss did not improve from 0.24486\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2556 - accuracy: 0.8908\n",
            "Epoch 87/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.8920\n",
            "Epoch 87: loss did not improve from 0.24486\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2508 - accuracy: 0.8920\n",
            "Epoch 88/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.8942\n",
            "Epoch 88: loss improved from 0.24486 to 0.23757, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2376 - accuracy: 0.8942\n",
            "Epoch 89/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.8933\n",
            "Epoch 89: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2425 - accuracy: 0.8932\n",
            "Epoch 90/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2501 - accuracy: 0.8919\n",
            "Epoch 90: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2501 - accuracy: 0.8919\n",
            "Epoch 91/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.8931\n",
            "Epoch 91: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2407 - accuracy: 0.8931\n",
            "Epoch 92/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.8941\n",
            "Epoch 92: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2388 - accuracy: 0.8941\n",
            "Epoch 93/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.8933\n",
            "Epoch 93: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2408 - accuracy: 0.8933\n",
            "Epoch 94/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.8925\n",
            "Epoch 94: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2420 - accuracy: 0.8925\n",
            "Epoch 95/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.8923\n",
            "Epoch 95: loss did not improve from 0.23757\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2391 - accuracy: 0.8922\n",
            "Epoch 96/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.8941\n",
            "Epoch 96: loss improved from 0.23757 to 0.23395, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2340 - accuracy: 0.8941\n",
            "Epoch 97/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.8946\n",
            "Epoch 97: loss did not improve from 0.23395\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2342 - accuracy: 0.8946\n",
            "Epoch 98/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.8932\n",
            "Epoch 98: loss did not improve from 0.23395\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2388 - accuracy: 0.8932\n",
            "Epoch 99/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.8917\n",
            "Epoch 99: loss did not improve from 0.23395\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2423 - accuracy: 0.8918\n",
            "Epoch 100/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.8938\n",
            "Epoch 100: loss improved from 0.23395 to 0.22929, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 20ms/step - loss: 0.2293 - accuracy: 0.8937\n",
            "Epoch 101/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.8956\n",
            "Epoch 101: loss improved from 0.22929 to 0.22567, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2257 - accuracy: 0.8956\n",
            "Epoch 102/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2368 - accuracy: 0.8930\n",
            "Epoch 102: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2368 - accuracy: 0.8930\n",
            "Epoch 103/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2304 - accuracy: 0.8944\n",
            "Epoch 103: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2305 - accuracy: 0.8943\n",
            "Epoch 104/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.8939\n",
            "Epoch 104: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2291 - accuracy: 0.8939\n",
            "Epoch 105/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.8938\n",
            "Epoch 105: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2306 - accuracy: 0.8938\n",
            "Epoch 106/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2294 - accuracy: 0.8938\n",
            "Epoch 106: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2293 - accuracy: 0.8938\n",
            "Epoch 107/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.8947\n",
            "Epoch 107: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2279 - accuracy: 0.8947\n",
            "Epoch 108/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.8944\n",
            "Epoch 108: loss did not improve from 0.22567\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2260 - accuracy: 0.8944\n",
            "Epoch 109/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.8955\n",
            "Epoch 109: loss improved from 0.22567 to 0.22037, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2204 - accuracy: 0.8956\n",
            "Epoch 110/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.8946\n",
            "Epoch 110: loss did not improve from 0.22037\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2281 - accuracy: 0.8946\n",
            "Epoch 111/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.8932\n",
            "Epoch 111: loss did not improve from 0.22037\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2315 - accuracy: 0.8931\n",
            "Epoch 112/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.8939\n",
            "Epoch 112: loss did not improve from 0.22037\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2240 - accuracy: 0.8939\n",
            "Epoch 113/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2242 - accuracy: 0.8940\n",
            "Epoch 113: loss did not improve from 0.22037\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2242 - accuracy: 0.8940\n",
            "Epoch 114/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.8936\n",
            "Epoch 114: loss did not improve from 0.22037\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2266 - accuracy: 0.8936\n",
            "Epoch 115/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2202 - accuracy: 0.8943\n",
            "Epoch 115: loss improved from 0.22037 to 0.22020, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2202 - accuracy: 0.8943\n",
            "Epoch 116/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2171 - accuracy: 0.8949\n",
            "Epoch 116: loss improved from 0.22020 to 0.21705, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2171 - accuracy: 0.8949\n",
            "Epoch 117/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.8951\n",
            "Epoch 117: loss did not improve from 0.21705\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2206 - accuracy: 0.8951\n",
            "Epoch 118/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.8945\n",
            "Epoch 118: loss did not improve from 0.21705\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2252 - accuracy: 0.8945\n",
            "Epoch 119/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.8933\n",
            "Epoch 119: loss did not improve from 0.21705\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2259 - accuracy: 0.8933\n",
            "Epoch 120/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.8951\n",
            "Epoch 120: loss did not improve from 0.21705\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2174 - accuracy: 0.8951\n",
            "Epoch 121/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2106 - accuracy: 0.8959\n",
            "Epoch 121: loss improved from 0.21705 to 0.21067, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2107 - accuracy: 0.8958\n",
            "Epoch 122/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2167 - accuracy: 0.8953\n",
            "Epoch 122: loss did not improve from 0.21067\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2166 - accuracy: 0.8953\n",
            "Epoch 123/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.8950\n",
            "Epoch 123: loss did not improve from 0.21067\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2197 - accuracy: 0.8950\n",
            "Epoch 124/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.8930\n",
            "Epoch 124: loss did not improve from 0.21067\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2296 - accuracy: 0.8930\n",
            "Epoch 125/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.8948\n",
            "Epoch 125: loss did not improve from 0.21067\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2179 - accuracy: 0.8948\n",
            "Epoch 126/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.8963\n",
            "Epoch 126: loss improved from 0.21067 to 0.20947, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2095 - accuracy: 0.8963\n",
            "Epoch 127/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.8959\n",
            "Epoch 127: loss did not improve from 0.20947\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2112 - accuracy: 0.8959\n",
            "Epoch 128/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.8948\n",
            "Epoch 128: loss did not improve from 0.20947\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2192 - accuracy: 0.8948\n",
            "Epoch 129/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.8947\n",
            "Epoch 129: loss did not improve from 0.20947\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2233 - accuracy: 0.8947\n",
            "Epoch 130/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.8945\n",
            "Epoch 130: loss did not improve from 0.20947\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2185 - accuracy: 0.8945\n",
            "Epoch 131/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2077 - accuracy: 0.8968\n",
            "Epoch 131: loss improved from 0.20947 to 0.20777, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2078 - accuracy: 0.8967\n",
            "Epoch 132/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.8957\n",
            "Epoch 132: loss did not improve from 0.20777\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2125 - accuracy: 0.8957\n",
            "Epoch 133/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.8956\n",
            "Epoch 133: loss did not improve from 0.20777\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2137 - accuracy: 0.8956\n",
            "Epoch 134/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.8947\n",
            "Epoch 134: loss did not improve from 0.20777\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2207 - accuracy: 0.8946\n",
            "Epoch 135/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.8935\n",
            "Epoch 135: loss did not improve from 0.20777\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2189 - accuracy: 0.8935\n",
            "Epoch 136/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.8960\n",
            "Epoch 136: loss did not improve from 0.20777\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2099 - accuracy: 0.8960\n",
            "Epoch 137/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.8968\n",
            "Epoch 137: loss improved from 0.20777 to 0.20389, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2039 - accuracy: 0.8968\n",
            "Epoch 138/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.8969\n",
            "Epoch 138: loss did not improve from 0.20389\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2057 - accuracy: 0.8969\n",
            "Epoch 139/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.8943\n",
            "Epoch 139: loss did not improve from 0.20389\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2167 - accuracy: 0.8943\n",
            "Epoch 140/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.8950\n",
            "Epoch 140: loss did not improve from 0.20389\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2173 - accuracy: 0.8950\n",
            "Epoch 141/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.8946\n",
            "Epoch 141: loss did not improve from 0.20389\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2148 - accuracy: 0.8946\n",
            "Epoch 142/150\n",
            "1002/1003 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.8943\n",
            "Epoch 142: loss did not improve from 0.20389\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2114 - accuracy: 0.8943\n",
            "Epoch 143/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.8968\n",
            "Epoch 143: loss improved from 0.20389 to 0.20165, saving model to next_words.h5\n",
            "1003/1003 [==============================] - 21s 21ms/step - loss: 0.2017 - accuracy: 0.8968\n",
            "Epoch 144/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2082 - accuracy: 0.8952\n",
            "Epoch 144: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2082 - accuracy: 0.8952\n",
            "Epoch 145/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.8946\n",
            "Epoch 145: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2158 - accuracy: 0.8946\n",
            "Epoch 146/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2118 - accuracy: 0.8956\n",
            "Epoch 146: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2117 - accuracy: 0.8956\n",
            "Epoch 147/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.8962\n",
            "Epoch 147: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2060 - accuracy: 0.8962\n",
            "Epoch 148/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.8959\n",
            "Epoch 148: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2092 - accuracy: 0.8960\n",
            "Epoch 149/150\n",
            "1001/1003 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.8956\n",
            "Epoch 149: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2090 - accuracy: 0.8956\n",
            "Epoch 150/150\n",
            "1003/1003 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.8960\n",
            "Epoch 150: loss did not improve from 0.20165\n",
            "1003/1003 [==============================] - 20s 20ms/step - loss: 0.2071 - accuracy: 0.8960\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b88ac97c0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=150, batch_size=70, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "olF1GUcgdlm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "token = pickle.load(open('token.pkl', 'rb'))\n",
        "def Predict_NextWords(model, token, text):\n",
        "\n",
        "  sequence = token.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in token.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "1leaRKwydrWy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  input_text = input(\"Enter your input line: \")\n",
        "  \n",
        "  if input_text == \"0\":\n",
        "      print(\"Execution completed\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          input_text = input_text.split(\" \")\n",
        "          input_text = input_text[-3:]\n",
        "          print(input_text)\n",
        "        \n",
        "          Predict_NextWords(model, token, input_text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHMNTVBe20Q",
        "outputId": "55499f40-f19c-4d21-ea1d-b06906f5c860"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your input line: A Room With A\n",
            "['Room', 'With', 'A']\n",
            "1/1 [==============================] - 1s 951ms/step\n",
            "view\n",
            "Enter your input line: The father did not see it; the son acknowledged it, not by another\n",
            "['not', 'by', 'another']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "bow\n",
            "Enter your input line: Beyond them stood the\n",
            "['them', 'stood', 'the']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "unreliable\n",
            "Enter your input line: to our depressing\n",
            "['to', 'our', 'depressing']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "signora\n",
            "Enter your input line: that he is a\n",
            "['he', 'is', 'a']\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "socialist\n",
            "Enter your input line: And presumably he has\n",
            "['presumably', 'he', 'has']\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "brought\n",
            "Enter your input line: The little old\n",
            "['The', 'little', 'old']\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "lady\n",
            "Enter your input line: 0\n",
            "Execution completed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}